# PyTorch

- 量化，裁剪，融合
- 压缩剪枝，量化，融合

## https://paperswithcode.com/
这个网站可用来寻找带有代码的论文资源。

## Partial Convolutions
- Image Inpainting for Irregular Holes Using Partial Convolutions
- https://arxiv.org/abs/1804.07723
- https://github.com/NVIDIA/partialconv

分别提取前景和背景的域表征。部分卷积最早应用于图像填充(image inpainting)，在这里用来提取形状不规则的前景和背景的域表征，可以避免其他区域的信息泄露和默认填充方法的干扰。

## Octave Convolution
- https://github.com/d-li14/octconv.pytorch

OctConv就如同卷积神经网络（CNN）的“压缩器”。比如说一个经典的图像识别算法，换掉其中的传统卷积，在ImageNet上的识别精度能获得1.2%的提升，同时，只需要82%的算力和91%的存储空间。如果对精度没有那么高的要求，和原来持平满足了的话，只需要一半的浮点运算能力就够了。Octave表示的是音阶的八度，而本篇核心思想是通过对数据中低频信息减半从而达到加速卷积运算的目的。

ResNet[18,19]和DenseNet[24]通过向早期层添加快捷连接来改进网络拓扑结构，增强特征重用机制，缓解优化困难。ResNeXt[45]和ShuffleNet[47]使用稀疏连接的组卷积来减少通道间连接的冗余，使得在相同的计算预算下采用更深或更广的网络是可行的。Xception[10]和MobileNet[20,35]采用深度卷积，进一步降低了连接密度。除了这些人工设计的网络，研究人员还试图原子地为给定的任务找到最佳的网络拓扑。NAS[49]、PNAS[30]和AmoebaNet[34]成功地发现了比手工设计的网络表现更好的类型学。另一项工作重点是减少模型参数中的冗余。DSD[16]通过修剪低权重的连接来减少模型连接中的冗余。ThiNet[32]删除了基于其下一层计算的统计数据的卷积滤波器。然而，所有这些方法都忽略了特征图空间维数的冗余，这是由所提出的OctConv来解决的，使得OctConv正交并且与之前的方法互补。

## Atention-Guided CNN For Image Denoising
- https://github.com/hellloxiaotian/ADNet

四个模块：一个稀疏块（SB），一个特征增强块（FEB），一个注意力机制（AB）和一个重构块(RB)来进行图像去噪。

## SimCLR
研究者们构建了一种用于视觉表示的对比学习简单框架SimCLR，它不仅优于此前的所有工作，也优于最新的对比自监督学习算法，而且结构更加简单：既不需要专门的架构，也不需要特殊的存储库。研究者发现：

1. 多个数据增强方法组合对于对比预测任务产生有效表示非常重要。此外，与有监督学习相比，数据增强对于无监督学习更加有用；
2. 在表示和对比损失之间引入一个可学习的非线性变换可以大幅提高模型学到的表示的质量；
3. 与监督学习相比，对比学习得益于更大的批量和更多的训练步骤。

具体说来，这一框架包含四个主要部分：

1. 随机数据增强模块，可随机转换任何给定的数据示例，从而产生同一示例的两个相关视图，分别表示为`x_i`和`x_j`，我们将其视为正对；
2. 一个基本的神经网络编码器`f()`，从增强数据中提取表示向量；
3. 一个小的神经网络投射头`g()`，将表示映射到对比损失的空间；
4. 为对比预测任务定义的对比损失函数。

在社交网络上，该论文的作者之一，谷歌资深研究科学家Mohammad Norouzi对这一学习算法进行了最简单化的总结：

1. 随机抽取一个小批量
2. 给每个例子绘制两个独立的增强函数
3. 使用两种增强机制，为每个示例生成两个互相关联的视图
4. 让相关视图互相吸引，同时排斥其他示例

多种数据增强操作的组合是学习良好表示的关键。为了进一步展示颜色增强的重要性，研究者调整了颜色增强的强度。
