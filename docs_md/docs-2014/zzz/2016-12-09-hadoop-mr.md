title: Hadoop MapReduce
date: 2016-12-09
tags: [Hadoop,MapReduce]
---
MapReduce是一个使用简易的软件框架，基于它写出来的应用程序能够运行在由上千个商用机器组成的大型集群上，并以一种可靠容错的方式并行处理上T级别的数据集。

<!--more-->
## 输入与输出
MapReduce框架运转在`<key, value>`键值对上，框架把作业的输入看作一组`<key, value>`键值对，同样也产出一组`<key, value>`键值对做为作业的输出，这两组键值对的类型可能不同。框架需要对key和value的类进行序列化操作，因此这些类需要实现Writable接口。另外，为了方便框架执行排序操作，key类必须实现WritableComparable接口。一个MapReduce作业的输入和输出类型如下所示：

    (input) <k1, v1> -> map -> <k2, v2> -> combine -> <k2, v2> -> reduce -> <k3, v3> (output)

## Example WordCount
`WordCount.java`源代码：
```java
import java.io.IOException;
import java.util.StringTokenizer;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;

public class WordCount {

  public static class TokenizerMapper
       extends Mapper<Object, Text, Text, IntWritable>{
    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();

    public void map(Object key, Text value, Context context
                    ) throws IOException, InterruptedException {
      StringTokenizer itr = new StringTokenizer(value.toString());
      while (itr.hasMoreTokens()) {
        word.set(itr.nextToken());
        context.write(word, one);
      }
    }
  }

  public static class IntSumReducer
       extends Reducer<Text,IntWritable,Text,IntWritable> {
    private IntWritable result = new IntWritable();

    public void reduce(Text key, Iterable<IntWritable> values, Context context
                       ) throws IOException, InterruptedException {
      int sum = 0;
      for (IntWritable val : values) {
        sum += val.get();
      }
      result.set(sum);
      context.write(key, result);
    }
  }

  public static void main(String[] args) throws Exception {
    Configuration conf = new Configuration();
    String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
    if (otherArgs.length < 2) {
      System.err.println("Usage: wordcount <in> [<in>..] <out>");
      System.exit(2);
    }
    Job job = Job.getInstance(conf, "word count");
    job.setJarByClass(WordCount.class);
    job.setMapperClass(TokenizerMapper.class);
    job.setCombinerClass(IntSumReducer.class);
    job.setReducerClass(IntSumReducer.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    for (int i = 0; i < otherArgs.length - 1; ++i) {
      FileInputFormat.addInputPath(job, new Path(otherArgs[i]));
    }
    FileOutputFormat.setOutputPath(job, new Path(otherArgs[otherArgs.length - 1]));
    System.exit(job.waitForCompletion(true) ? 0 : 1);
  }
}
```

编译`WordCount.java`并构建`wc.jar`：
```shell
[root@c01 lab]# tar -zxf jdk-8u111-linux-x64.tar.gz -C /lab
[root@c01 lab]# echo $'export JAVA_HOME=/lab/jdk1.8.0_111' >> /etc/profile
[root@c01 lab]# echo $'export PATH=${JAVA_HOME}/bin:${PATH}' >> /etc/profile
[root@c01 lab]# echo $'export HADOOP_CLASSPATH=${JAVA_HOME}/lib/tools.jar' >> /etc/profile
[root@c01 lab]# source /etc/profile
[root@c01 hadoop-2.7.3]# ./bin/hadoop com.sun.tools.javac.Main WordCount.java
[root@c01 hadoop-2.7.3]# jar cf wc.jar WordCount*.class
```

准备数据并执行程序：
```shell
[root@c01 hadoop-2.7.3]# ./sbin/start-all.sh
[root@c01 hadoop-2.7.3]# ./bin/hadoop fs -mkdir -p /lab/wc
[root@c01 hadoop-2.7.3]# ./bin/hadoop fs -copyFromLocal /lab/solr/hadoop-2.7.3/input /lab/wc
[root@c01 hadoop-2.7.3]# ./bin/hadoop fs -ls -R /lab/wc/input
-rw-r--r--   1 root supergroup         38 2016-12-05 12:02 /lab/wc/input/file01
-rw-r--r--   1 root supergroup         53 2016-12-05 12:02 /lab/wc/input/file02
[root@c01 hadoop-2.7.3]# ./bin/hadoop fs -cat /lab/wc/input/file01
何剑你好！
今天天气真好。
[root@c01 hadoop-2.7.3]# ./bin/hadoop fs -cat /lab/wc/input/file02
保持好心态。
自律，自强，自信，坚持
[root@c01 hadoop-2.7.3]# ./bin/hadoop jar wc.jar WordCount /lab/wc/input /lab/wc/input /lab/wc/output
[root@c01 hadoop-2.7.3]# ./bin/hadoop fs -ls -R /lab/wc/output
-rw-r--r--   1 root supergroup          0 2016-12-05 14:43 /lab/wc/output/_SUCCESS
-rw-r--r--   1 root supergroup         99 2016-12-05 14:43 /lab/wc/output/part-r-00000
[root@c01 hadoop-2.7.3]# ./bin/hadoop fs -cat /lab/wc/output/part-r-00000
今天天气真好。 2
何剑你好！   2
保持好心态。  2
自律，自强，自信，坚持 2
[root@c01 hadoop-2.7.3]# ./bin/hadoop fs -rm -R /lab/wc/output
[root@c01 hadoop-2.7.3]# ./sbin/stop-all.sh
```

## WordCount源码分析
1) Map过程
Map过程需要继承Mapper类，并重写其map方法。通过在map方法中添加把key和value输出到控制台的代码，可以发现map方法中value存储的是文本文件中的一行，以回车符为行结束标记，而key为该行的首字母相对于文本文件的首地址的偏移量。然后StringTokenizer类将每一行拆分成为一个个的单词，并将`<word,1>`作为map方法的结果输出，其余的工作都交由MapReduce框架处理。

2) Reduce过程
Reduce过程需要继承Reducer类，并重写其reduce方法。Map过程输出`<key,values>`中key为单个单词，而values是对应单词的计数值所组成的列表，Map的输出就是Reduce的输入，所以reduce方法只要遍历values并求和，即可得到某个单词的总次数。

3) 执行MapReduce任务
在MapReduce中，由Job对象负责管理和运行一个计算任务，并通过Job的一些方法对任务的参数进行相关的设置。此处设置了使用TokenizerMapper完成Map过程中的处理和使用IntSumReducer完成Combine和Reduce过程中的处理。还设置了Map过程和Reduce过程的输出类型。任务的输出和输入路径则由命令行参数指定，并由FileInputFormat和FileOutputFormat分别设定。完成相应任务的参数设定后，即可调用`job.waitForCompletion()`方法执行任务。

## 特别数据类型介绍

Hadoop提供了如下的数据类型，这些数据类型都实现了WritableComparable接口，以便用这些类型定义的数据可以被序列化进行网络传输和文件存储，以及进行大小比较：

1. BooleanWritable：标准布尔型
2. ByteWritable：单字节数值
3. DoubleWritable：双字节数
4. FloatWritable：浮点数
5. IntWritable：整型数
6. LongWritable：长整型数
7. Text：使用UTF8格式存储的文本
8. NullWritable：当`<key,value>`中的key或value为空时使用

## 作业的输入
MapReduce框架根据作业的InputFormat来：

1. 检查作业输入的有效性。
2. 把输入文件切分成多个逻辑InputSplit实例，并把每一实例分别分发给一个Mapper。
3. 提供RecordReader的实现，这个RecordReader从逻辑InputSplit中获得输入记录，这些记录将由Mapper处理。

基于文件的InputFormat实现通常是FileInputFormat的子类，默认行为是按照输入文件的字节大小，把输入数据切分成逻辑分块`logical InputSplit`。其中输入文件所在的FileSystem的数据块尺寸是分块大小的上限。下限可以设置`mapred.min.split.size`的值。考虑到边界情况，对于很多应用程序来说，很明显按照文件大小进行逻辑分割是不能满足需求的。在这种情况下，应用程序需要实现一个RecordReader来处理记录的边界并为每个任务提供一个逻辑分块的面向记录的视图。TextInputFormat是默认的InputFormat。

1) InputSplit
InputSplit是一个单独的Mapper要处理的数据块。一般的InputSplit是字节样式输入，然后由RecordReader处理并转化成记录样式。FileSplit是默认的InputSplit。它把`map.input.file`设定为输入文件的路径，输入文件是逻辑分块文件。

2) RecordReader
RecordReader从InputSlit读入`<key, value>`对。一般的，RecordReader把由InputSplit提供的字节样式的输入文件，转化成由Mapper处理的记录样式的文件。因此RecordReader负责处理记录的边界情况和把数据表示成`keys/values`对形式。默认的RecordReader是LineRecordReader。

## 作业的输出
OutputFormat描述MapReduce作业的输出样式。MapReduce框架根据作业的OutputFormat来：

1. 检验作业的输出，例如检查输出路径是否已经存在。
2. 提供一个RecordWriter的实现，用来输出作业结果。输出文件保存在FileSystem上。

TextOutputFormat是默认的OutputFormat。RecordWriter写`<key, value>`对到输出文件。RecordWriter的实现把作业的输出结果写到FileSystem。

## 其他有用的特性
1) Counters
Counters是多个由MapReduce框架或者应用程序定义的全局计数器。每一个Counter可以是任何一种Enum类型。同一特定Enum类型的Counter可以汇集到一个组，其类型为`Counters.Group`。

应用程序可以定义任意(Enum类型)的Counters并且可以通过map或者reduce方法中的`Reporter.incrCounter(Enum, long)`或者`Reporter.incrCounter(String, String, long)`更新。之后框架会汇总这些全局counters。

2) DistributedCache
DistributedCache可将具体应用相关的、大尺寸的、只读的文件有效地分布放置。DistributedCache是MapReduce框架提供的功能，能够缓存应用程序所需的文件，包括文本、档案文件和jar文件等。应用程序在JobConf中通过url(hdfs://)指定需要被缓存的文件。DistributedCache假定由hdfs://格式url指定的文件已经在FileSystem上了。

MapRedcue框架在作业所有任务执行之前会把必要的文件拷贝到slave节点上。它运行高效是因为每个作业的文件只拷贝一次并且为那些没有文档的slave节点缓存文档。DistributedCache根据缓存文档修改的时间戳进行追踪。在作业执行期间，当前应用程序或者外部程序不能修改缓存文件。DistributedCache可以分发简单的只读数据或文本文件，也可以分发复杂类型的文件例如归档文件和jar文件。归档文件(zip、tar、tgz和tar.gz文件)在slave节点上会被解档。

用户可以通过设置`mapreduce.job.cache.{files|archives}`来分发文件。如果要分发多个文件，可以使用逗号分隔文件所在路径。也可以利用API来设置该属性：`Job.addCacheFile(URI)`、`Job.addCacheArchive(URI)`和`Job.setCacheFiles(URI[])`、`Job.setCacheArchives(URI[])`。程序中URI形式是`hdfs://host:port/absolute-path#link-name`，其中`#link-name`让DistributedCache在当前工作目录下创建到缓存文件的符号链接，也可以通过命令行选项`-cacheFile/-cacheArchive`实现。

DistributedCache可在MapReduce任务中作为一种基础软件分发机制使用。它可以被用于分发jar包和本地库。`Job.addArchiveToClassPath(Path)`和`Job.addFileToClassPath(Path)`API能够被用于缓存文件和jar包，并把它们加入子jvm的classpath。也可以通过设置配置属性`mapreduce.job.classpath.{files|archives}`达到相同的效果。

## 参考资料：
- [MapReduce Tutorial·中文·旧版](http://hadoop.apache.org/docs/r1.0.4/cn/mapred_tutorial.html)
- [MapReduce Tutorial·英文·最新](http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html)