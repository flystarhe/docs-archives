# https://arxiv.org/abs/1807.06821v1
Computed Tomography Image Enhancement using 3D Convolutional Neural Network

三维卷积神经网络在CT图像增强中的应用

## 1 Introduction
Computed tomography (CT) is a widely used screening and diagnostic tool that provides detailed anatomical information on patients. Its ability to resolve small objects, such as nodules that are 1-30 mm in size, makes the modality indispensable in performing tasks such as lung cancer screening and colonography. However, the variation in image resolution of CT screening due to differences in radiation dose and slice thickness hinders the radiologist’s ability to discern subtle suspicious findings. Thus, it is highly desirable to develop an approach that enhances lower resolution CT scans by increasing the detail and sharpness of borders to mimic higher resolution acquisitions [1].

计算机断层扫描（CT）是一种广泛使用的筛查和诊断工具，可提供患者的详细解剖信息。它能够分辨小物体，例如1-30毫米大小的结节，使得这种方式在执行肺癌筛查和结肠成像等任务时不可或缺。然而，由于辐射剂量和切片厚度的差异导致的CT筛查图像分辨率的变化阻碍了放射科医师辨别细微可疑结果的能力。因此，非常希望开发一种方法，通过增加边界的细节和清晰度来模拟更高分辨率的采集，从而增强低分辨率CT扫描[1]。

Super-resolution (SR) is a class of techniques that increase the resolution of an imaging system [2] and has been widely applied on natural images and is increasingly being explored in medical imaging. Traditional SR methods use linear or non-linear functions (e.g., bilinear/bicubic interpolation and example-based methods [3,4]) to estimate and simulate image distributions. These methods, however, produce blurring and jagged edges in images, which introduce artifacts and may negatively impact the ability of computer-aided detection (CAD) systems to detect subtle nodules. Recently, deep learning, especially convolutional neural networks (CNN), has been shown to extract high-dimensional and nonlinear information from images that results in a much improved super-resolution output. One example is the super-resolution convolutional neural network (SRCNN) [5]. SRCNN learns an end-to-end mapping from low- to high-resolution images. In [6,7], the authors applied and evaluated the SRCNN method to improve the image quality of magnified images in chest radiographs and CT images. Moreover, [9] introduced an efficient sub-pixel convolution network (ESPCN), which was shown to be more computationally efficient than SRCNN. In [10], the authors proposed a SR method that utilizes a generative adversarial network (GAN), resulting in images have better perceptual quality compared to SRCNN. All these methods were evaluated using 2D images. However, for medical imaging modalities that are volumetric, such as CT, a 2D convolution ignores the correlation between slices. We propose a 3DECNN architecture, which executes a series of 3D convolutions on the volumetric data. We measure performance using two image quality metrics: peak signal-to-noise ratio (PSNR) and structural similarity (SSIM). Our approach achieves significant improvement compared with improved SRCNN approach (FSRCNN) [8] and [9] on both metrics.

超分辨率（SR）是一类提高成像系统分辨率的技术[2]，已广泛应用于自然图像，并且越来越多地被用于医学成像。传统的SR方法使用线性或非线性函数（例如，双线性/双三次插值和基于实例的方法[3,4]）来估计和模拟图像分布。然而，这些方法在图像中产生模糊和锯齿状边缘，这会引入伪像并且可能对计算机辅助检测（CAD）系统检测细微结节的能力产生负面影响。最近，深度学习，特别是卷积神经网络（CNN），已经被证明可以从图像中提取高维度和非线性信息，从而产生大大提高的超分辨率输出。一个例子是超分辨率卷积神经网络（SRCNN）[5]。SRCNN学习从低分辨率图像到高分辨率图像的端到端映射。在[6,7]中，作者应用并评估了SRCNN方法，以提高胸片和CT图像中放大图像的图像质量。此外，[9]引入了一种有效的亚像素卷积网络（ESPCN），其被证明比SRCNN在计算上更有效。在[10]中，作者提出了一种利用生成对抗网络（GAN）的SR方法，与SRCNN相比，图像具有更好的感知质量。使用2D图像评估所有这些方法。然而，对于体积的医学成像模态，例如CT，2D卷积忽略了切片之间的相关性。我们提出了一种3DECNN架构，它对体积数据执行一系列3D卷积。我们使用两个图像质量度量来衡量性能：峰值信噪比（PSNR）和结构相似性（SSIM）。与改进的SRCNN方法（FSRCNN）[8]和[9]相比，我们的方法在两个指标上都取得了显着的进步。

## 2 Method

### 2.1 Overview
For each slice in the CT volume, our task is to generate a high-resolution image IHR from a low-resolution image ILR. Our approach can be divided into two phases: model training and inference. In the model training phase, we first downsample a given image I to obtain the low-resolution image ILR. We then use the original data as the high-resolution images IHR to train our proposed 3DECNN network. In the model inference phase, we use a previously unseen low-resolution CT volume as input to the trained 3DECNN model and generate a super resolution image ISR.

对于CT体积中的每个切片，我们的任务是从低分辨率图像ILR生成高分辨率图像IHR。我们的方法可以分为两个阶段：模型训练和推理。在模型训练阶段，我们首先对给定图像I进行下采样以获得低分辨率图像ILR。然后，我们使用原始数据作为高分辨率图像IHR来训练我们提出的3DECNN网络。在模型推断阶段，我们使用先前未见过的低分辨率CT体积作为训练的3DECNN模型的输入并生成超分辨率图像ISR。

### 2.2 Formulation
For CT images, spatial correlations exist across three dimensions. As such, the key to generating high-quality SR images is to make full use of available information along all dimensions. Thus, we apply cube-shaped filters on the input CT slices and slides these filters through all three dimensions of the input. Our model architecture is illustrated in Fig. 1. This filtering procedure is repeated in 3 stacked layers. After the 3D filtering process, a 3D deconvolution is used to reconstruct images and up-sample them to larger ones. The output of this 3D deconvolution is a reconstructed SR 3D volume. However, to compare with other SR methods such as SRCNN and ESPCN, which produces 2D outputs, we transform our 3D volume into a 2D output. As such, we add a final convolution layer to smooth pixels into a 2D slice, which is then compared to the outputs of the other methods. In the following paragraphs, we describe mathematical details of our 3DECNN architecture.

对于CT图像，空间相关性存在于三个维度上。因此，生成高质量SR图像的关键是充分利用所有维度的可用信息。因此，我们在输入CT切片上应用立方体形滤波器，并将这些滤波器滑过输入的所有三个维度。我们的模型架构如图1所示。该滤波过程在3个堆叠层中重复。在3D滤波过程之后，3D去卷积用于重建图像并将它们上采样到更大的图像。该3D反卷积的输出是重建的SR3D体积。然而，为了与产生2D输出的SRCNN和ESPCN等其他SR方法进行比较，我们将3D体积转换为2D输出。因此，我们添加最终卷积层以将像素平滑到2D切片中，然后将其与其他方法的输出进行比较。在以下段落中，我们描述了3DECNN架构的数学细节。

3D Convolutional Layers. In this work, we incorporate the feature extraction optimizations into the training/learning procedure of convolution kernels. The original CT images are normalized to values between `[0,1]`. The first CNN layer takes a normalized CT image (represented as a 3-D tensor) as input and generates multiple 3-D tensors (feature maps) as output by sliding the cube-shaped filters (convolution kernels), which are sized of `k1×k2×k3`, across inputs. We define convolution input tensor notations as `<N, Cin, H, W>` and output `<N, Cout, H, W>`, in which `Ci` stands for the number of 3-D tensors and `<N, H, W>` stands for the feature map block’s thickness, height, and width, respectively. Subsequent convolution layers take the previous layer’s output feature maps as input, which are in a 4-D tensor. Convolution kernels are in a dimension of `<Cin, Cout, k1, k2, k3>`. The sliding stride parameter `<s>` defines how many pixels to skip between each adjacent convolution on input feature maps.

3D卷积层。在这项工作中，我们将特征提取优化结合到卷积核的训练/学习过程中。将原始CT图像归一化为`[0,1]`之间的值。第一个CNN层采用归一化CT图像（表示为3-D张量）作为输入，并通过滑动立方体形状的滤波器（卷积核）生成多个3-D张量（特征图）作为输出，其尺寸为`k1×k2×k3`，跨输入。我们将卷积输入张量符号定义为`<N,Cin,H,W>`并输出`<N,Cout,H,W>`，其中`Ci`代表3-D张量的数量并且`<N，H，W>`分别代表特征图块的厚度，高度和宽度。随后的卷积层将前一层的输出特征图作为输入，它们处于4-D张量中。卷积核的维度为`<Cin,Cout,k1,k2,k3>`。滑动步幅参数`<s>`定义输入特征图上每个相邻卷积之间要跳过的像素数。

Deconvolution layer. In traditional image processing, a reverse feature extraction procedure is typically used to reconstruct images. Specifically, design functions such as linear interpolation, are used to up-scale images and also average overlapped output patches to generate the final SR image. In this work, we utilize deconvolution to achieve image up-sampling and reconstruct feature information from previous layers’ outputs at the same time. Deconvolution can be thought of as a transposed convolution. Deconvolution operations up-sample input feature maps by multiplying each pixel with cubic filters and summing up overlap outputs of adjacent filters’ output [11]. Activation functions are used to apply an element-wise non-linear transformation on the convolution or deconvolution output tensors. In this work, we use ReLU as the activation function.

反卷积层。在传统的图像处理中，反向特征提取过程通常用于重建图像。具体地，诸如线性插值的设计功能用于放大图像并且还用于平均重叠的输出贴片以生成最终的SR图像。在这项工作中，我们利用反卷积实现图像上采样，同时重建前一层输出的特征信息。解卷积可以被认为是转置卷积。反卷积操作通过将每个像素与立方滤波器相乘并对相邻滤波器输出的重叠输出求和来上采样输入特征图[11]。激活函数用于在卷积或反卷积输出张量上应用逐元素的非线性变换。在这项工作中，我们使用ReLU作为激活功能。

Hyperparameters. There are four hyperparameters that have an influence on model performance: number of feature layers, feature map depth, number of convolution kernels, and size of kernels. The number of feature extraction layers `<l>` determines the upper-bound complexity in features that the CNN can learn from images. The feature map depth `<n>` is the number of CT slices that are taken in together to generate one SR image. The number of convolution kernels `<f>` decides the number of total feature maps in a layer and thus decides the maximum information that can be represented in the output of this layer. The size of convolution and deconvolution kernels `<k>` decides the visible scope that the filter can see in the input CT image or feature maps. Given the impact of each hyperparameter, we performed a grid search of the hyperparameter space to find the best combination of `<n,l,f,k>` for our 3DECNN model.

超参数。有四个超参数对模型性能有影响：特征图层数，特征图深度，卷积内核数和内核大小。特征提取层`<l>`的数量决定了CNN可以从图像中学习的特征的上限复杂性。特征映射深度`<n>`是一起生成一个SR图像的CT切片的数量。卷积内核的数量`<f>`决定了一个层中总特征图的数量，从而决定了可以在该层的输出中表示的最大信息。卷积和反卷积内核`<k>`的大小决定了滤镜在输入CT图像或特征图中可以看到的可见范围。考虑到每个超参数的影响，我们对超参数空间进行了网格搜索，以找到我们的3DECNN模型的`<n,l,f,k>`的最佳组合。

Loss function. Peak signal-to-noise ratio (PSNR) is the most commonly used metric to measure the quality of reconstructed lossy images in all kinds of imaging systems. A higher PSNR generally indicates a higher quality of the reconstruction image. PSNR is defined as the log on the division of the max pixel value over mean squared root. Therefore, we directly use the squared mean error function as our loss function. In addition, the target loss function is minimized using stochastic gradient descent with the back-propagation algorithm [13].

损失函数。峰值信噪比（PSNR）是测量各种成像系统中重建有损图像质量的最常用度量。较高的PSNR通常表示重建图像的较高质量。PSNR定义为最大像素值除以均方根的对数。因此，我们直接使用平方均值误差函数作为我们的损失函数。此外，利用反向传播算法[13]，使用随机梯度下降最小化目标损失函数。

## 3 Experiments and Results
In this section, we first introduce the experiment setup, including dataset and data preparation. Then we show the design space of the hyper-parameters, at which time we show how to explore different CNN architectures and find the best model. Subsequently, we compare our method with recent state-of-the-art work and demonstrate the performance improvement. Lastly, we present examples of the generated SR CT images using our proposed method and previous state-ofthe-art results.

在本节中，我们首先介绍实验设置，包括数据集和数据准备。然后我们展示了超参数的设计空间，同时我们展示了如何探索不同的CNN架构并找到最佳模型。随后，我们将我们的方法与最近的最新工作进行比较，并展示性能改进。最后，我们使用我们提出的方法和先前的最新结果提供生成的SRCT图像的示例。

### 3.1 Experiment setup
Dataset. We use the public available Lung Image Database Consortium image collection (LIDC) dataset for this study [12], which consists of low- and diagnostic-dose thoracic CT scans. These scans have a wide range of slice thickness ranging from 0.6 to 5 mm. And the pixel spacing in axial view (x-y direction) ranges from 0.4609 to 0.9766 mm. We randomly select 100 scans out of a total of 1018 cases from the LIDC dataset, result in a total consisting of 20672 slices. The selected CT scans are then randomized into four folds with similar size. Two folds are used for training, and the remaining two folds are used for validation and test, respectively.

数据集。我们使用公共可用的肺部图像数据库联盟图像采集（LIDC）数据集进行本研究[12]，其中包括低剂量和诊断剂量的胸部CT扫描。这些扫描的切片厚度范围很宽，范围从0.6到5毫米。并且轴向视图（x-y方向）的像素间距在0.4609至0.9766mm的范围内。我们从LIDC数据集的总共1018个案例中随机选择100个扫描，结果总共包含20672个切片。然后将选择的CT扫描随机分成具有相似大小的四个折叠。两个折叠用于训练，其余两个折叠分别用于验证和测试。

Data preprocessing. For each CT scan, we first downsample it on axial view by the desired scaling factor (set 3 in our experiment) to form the LR images. Then the corresponding HR images are ground truth images.

数据预处理。对于每次CT扫描，我们首先在轴向视图上按照所需的比例因子（在我们的实验中设置3）对其进行下采样，以形成LR图像。然后相应的HR图像是地面真实图像。

Hyperparameter tuning `<n,l,f,k>`. We choose the four most influential parameters to explore in our experiment and discuss, which is feature depth `<n>`, number of layers `<l>`, number of filters `<f>` and filter kernel size `<k>`. The effect of the feature depth `<n>` is shown in Fig. 2(a). It presents the training curves of three different 3DECNN architectures, in which their `<l,f,k>` are the same and `<n>` varies in `[3, 5, 9]`. Among the three configurations, `n=3` has a better average PSNR than the others. The effect of the number of layers `<l>` is shown Fig. 2(b), which demonstrates that a deeper CNN may not always be better. With fixed `<n,f,k>` and varying `<l>` ∈ `[1,3,5,8]`, here `<l>` indicate the number of convolutional layers before the deconvolution process. we can observe apparent different performance on the training curves. We determine that `l=3` achieves higher average PSNR. The effect of the number of filters `<f>` is shown in Fig. 2(c), in which we fix `<n,l,k>` and choose `<f>` in four collections. An apparent drop in PSNR is seen when `<f>` chooses the too small configuration `<16,16,16,32,1>`. `<64,64,64,32,1>` and `<64,64,32,32,1>` has approximately the same PSNR (28.66 vs. 28.67) so we choose latter one to save training time. The effect of the filter kernel size `<k>` is shown in Fig. 2(d), in which we fix `<n,l,f>` and vary k in the collection of `[3,5,9]`. Experiment result proves that `k=3` achieves the best PSNR. The PSNR decrease with filter kernel size demonstrate that relatively remote pixels contribute less to feature extraction and bring much signal noise to the final result.

超参数调整`<n，l，f，k>`。我们在实验和讨论中选择了四个最有影响力的参数，即特征深度`<n>`，层数`<l>`，过滤器数量`<f>`和过滤器内核大小`<k>`。特征深度`<n>`的影响如图2（a）所示。它给出了三种不同3DECNN架构的训练曲线，其中`<l，f，k>`是相同的，`<n>`在`[3,5,9]`中变化。在这三种配置中，`n=3`具有比其他配置更好的平均PSNR。层数`<l>`的影响如图2（b）所示，这表明更深的CNN可能并不总是更好。对于固定的`<n，f，k>`和变化的`<l>`∈`[1,3,5,8]`，这里`<l>`表示去卷积过程之前的卷积层数。我们可以在训练曲线上观察到明显不同的表现。我们确定`l=3`实现了更高的平均PSNR。过滤器数量`<f>`的效果如图2（c）所示，其中我们修复了`<n，l，k>`并在四个集合中选择`<f>`。当`<f>`选择太小的配置`<16,16,16,32,1>`时，可以看到PSNR明显下降。`<64,64,64,32,1>`和`<64,64,32,32,1>`具有大致相同的PSNR（28.66对28.67），所以我们选择后者来节省训练时间。滤波器内核大小`<k>`的效果如图2（d）所示，其中我们修复`<n，l，f>`并在`[3,5,9]`的集合中改变`<k>`。实验结果证明`k=3`达到了最佳PSNR。PSNR随滤波器内核尺寸的减小表明相对较远的像素对特征提取的贡献较小，并为最终结果带来了很多信号噪声。

Final model. For the final design, we set `<n,l,(f1,k1),(f2,k2),(f3,k3),(f4_deconv,k4_deconv),(f5,k5)>: <5,3,(64,3),(64,3),(32,3),(32,3),(1,3)>`. We set the learning rate α as `10^3` for this design and achieve a good convergence. We implemented our 3DECNN model using Pytorch and trained/validated our model on a workstation with a NVIDIA Tesla K40 GPU. The training process took roughly 10 hours.

最终模型。对于最终设计，我们设置`<n，l，（f1，k1），（f2，k2），（f3，k3），（f4_deconv，k4_deconv），（f5，k5）>: <5,3，（64,3），（64,3），（32,3），（32,3），（1,3）>`。我们将学习率α设置为`10^3`，以实现良好的收敛。我们使用Pytorch实现了我们的3DECNN模型，并在具有NVIDIA Tesla K40 GPU的工作站上训练/验证了我们的模型。培训过程大约需要10个小时。

We compare the proposed model to bicubic interpolation and two existing thestate-of-the-art deep learning methods for super resolution image enhancement: 1) FSRCNN [8] and 2) ESPCN [9]. We reimplemented both methods, retraining and testing them in the manner as our proposed method. Both the FSRCNN-s and the FSRCNN architectures used in [8] are compared here. A paired t-test is adopted to determine whether a statistically significant difference exists in mean measurements of PSNR and SSIM when comparing 3DECNN to bicubic, FSRCNN, and ESPCN. Table 1 shows the mean and standard deviation for the four methods in PSNR and SSIM using 5,168 test slices. The paired t-test results show that the proposed method has significantly higher mean PSNR, and mean differences are 2.0183 dB (p-value < 2.2e − 16), 0.8357 dB (p-value < 2.2e−16), 0.5406 dB (p-value < 2.2e−16), and 0.4318 dB (p-value < 2.2e−16) for bicubic, FSRCNN-s, FSRCNN and ESPCN, respectively. It also shows that out model has significantly higher SSIM, and the mean differences are 0.0389 (p-value < 2.2e−16), 0.0136 (p-value < 2.2e−16), 0.0098 (p-value < 2.2e−16), and 0.0080 (p-value < 2.2e − 16). To subjectively measure the image perceived quality, we also visualize and compare the enhanced images in Fig. 3. The zoomed areas in the figure are lung nodules. As the figures shown, our approach achieved better perceived quality compared to other methods.

我们将所提出的模型与双三次插值和两种现有的用于超分辨率图像增强的现有深度学习方法进行比较：1）FSRCNN[8]和2）ESPCN[9]。我们重新实现了这两种方法，以我们提出的方法重新训练和测试它们。这里比较了[8]中使用的FSRCNN-s和FSRCNN架构。采用配对t检验来确定当比较3DECNN与双三次插值，FSRCNN和ESPCN时，PSNR和SSIM的平均测量值是否存在统计学上显着的差异。表1显示了使用5,168个测试切片的PSNR和SSIM中四种方法的平均值和标准偏差。配对t检验结果表明，该方法具有明显更高的平均PSNR，平均差异为2.0183dB（p<2.2e-16），0.8357dB（p<2.2e-16），0.5406dB（对于双三次插值，FSRCNN-s，FSRCNN和ESPCN，p<2.2e-16）和0.4318dB（p<2.2e-16）。它还表明模型具有显着更高的SSIM，平均差异为0.0389（p<2.2e-16），0.0136（p<2.2e-16），0.0098（p<2.2e-16）），和0.0080（p<2.2e-16）。为了主观测量图像感知质量，我们还可视化和比较图3中的增强图像。图中的缩放区域是肺结节。如图所示，与其他方法相比，我们的方法获得了更好的感知质量。

## 4 Discussion and Future work
We present the results of our proposed 3DECNN approach to improve the image quality of CT studies that are acquired at varying, lower resolutions. Our method achieves a significant improvement compared to existing state-of-art deep learning methods in PSNR (mean improvement of 0.43dB and p-value < 2.2e − 16) and SSIM (mean improvement of 0.008 and p-value < 2.2e − 16). We demonstrate our proposed work by enhancing large slice thickness scans, which can be potentially applied to clinical auxiliary diagnosis of lung cancer. As future work, we explore how our approach can be extended to perform image normalization and enhancement of ultra low-dose CT images (studies that are acquired at 25% or 50% dose compared to current low-dose images) with the goal of producing comparable image quality while reducing radiation exposure to patients.

我们提出了我们提出的3DECNN方法的结果，以改善在不同的较低分辨率下获得的CT研究的图像质量。与PSNR中现有的最先进的深度学习方法（平均改善0.43dB）和SSIM（平均改善0.008）相比，我们的方法取得了显着的改进。我们通过增强大切片厚度扫描来证明我们提出的工作，这可以潜在地应用于肺癌的临床辅助诊断。作为未来的工作，我们探索如何扩展我们的方法以执行图像标准化和超低剂量CT图像的增强（以与当前低剂量图像相比以25％或50％剂量获得的研究），目标是生产可比较的图像质量，同时减少患者的辐射照射。